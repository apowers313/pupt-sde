<Prompt
  name="sde-debug-root-cause"
  title="Debug Root Cause"
  description="Systematic debugging and root cause analysis using evidence-based investigation"
  version="1.0.0"
  tags={["sde", "debugging", "maintenance", "rca"]}
  noRole
  noFormat
  noConstraints
  noSuccessCriteria
  noGuardrails
>
  <Ask.Editor
    name="bugDescription"
    label="Bug Description / Error Information"
    description="Paste error messages, stack traces, logs, bug description, or reproduction steps"
    required
    silent
    language="text"
  />

  <Ask.Editor
    name="relevantCode"
    label="Relevant Code (Optional)"
    description="Paste the code where the bug occurs, or leave empty to investigate the codebase"
    silent
    language="typescript"
  />

  <Ask.MultiSelect
    name="analysisDepth"
    label="Analysis Focus Areas"
    description="Select specific areas to investigate (leave empty for comprehensive analysis)"
    silent
    options={[
      { value: "immediate", label: "Immediate Cause - What triggered the failure" },
      { value: "root", label: "Root Cause - Why it happened" },
      { value: "chain", label: "Failure Chain - Full causal sequence" },
      { value: "prevention", label: "Prevention - How to avoid similar bugs" },
      { value: "monitoring", label: "Monitoring - Detection strategies" }
    ]}
  />

  <Ask.Select
    name="bugSeverity"
    label="Bug Severity (Optional)"
    description="Helps prioritize the analysis depth and urgency of recommendations"
    default="unknown"
    silent
    options={[
      { value: "critical", label: "Critical - Production down, data loss, or security breach" },
      { value: "high", label: "High - Major feature broken, workaround exists" },
      { value: "medium", label: "Medium - Degraded functionality, non-blocking" },
      { value: "low", label: "Low - Minor issue, cosmetic, or edge case" },
      { value: "unknown", label: "Unknown - Severity not yet determined" }
    ]}
  />

  <Ask.Confirm
    name="includeSystemContext"
    label="Include system-wide analysis?"
    description="Search for similar patterns across the codebase and check for related issues"
    silent
  />

  <Role
    preset="engineer"
    experience="expert"
    expertise={["debugging", "root cause analysis", "system diagnostics", "incident investigation"]}
    traits={["methodical", "evidence-driven", "thorough", "skeptical of assumptions", "actively seeks disconfirming evidence"]}
    domain="software debugging and failure analysis"
  >
    <Specialization
      areas={["systematic debugging", "fault tree analysis", "fishbone/ishikawa analysis", "postmortem investigation", "5 whys methodology", "backward tracing", "binary search debugging", "delta debugging", "explanation-based debugging"]}
      level="expert"
    />
  </Role>

  <Objective
    primary="Identify the root cause of the reported bug through systematic, evidence-based investigation and provide actionable fix recommendations"
    secondary={[
      "Trace the complete failure chain from symptom back to origin",
      "Distinguish between immediate triggers, contributing factors, and underlying root causes",
      "Provide fix recommendations with risk assessment, implementation guidance, and verification steps",
      "Develop prevention strategies addressing both the specific bug and systemic weaknesses"
    ]}
    metrics={[
      "Root cause identified with supporting evidence from code, logs, or reproduction",
      "Failure chain documented from trigger through intermediate steps to observed symptom",
      "Fix recommendations include implementation approach, side effects, and verification steps",
      "Prevention strategies address both this specific bug and the class of bugs it represents"
    ]}
  />

  <Task
    verb="Investigate"
    subject="a reported bug using systematic root cause analysis"
    objective="to trace from observed symptoms to the underlying cause, then provide actionable fix and prevention recommendations"
    scope="comprehensive"
    complexity="complex"
  >
    Perform a systematic root cause analysis of the reported bug. Use the scientific method: observe symptoms, form hypotheses, test them with evidence, and verify conclusions. Follow the iron law of debugging: NO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST. Trace the failure backward from where it manifests to where it originates. Produce a comprehensive analysis that enables confident, targeted remediation.
  </Task>

  <Contexts>
    <Context type="situational" label="Analysis Timestamp" priority="helpful">
      Analysis initiated: <DateTime />
    </Context>

    <Context type="data" label="Bug Report" priority="critical" preserveFormatting>
      {bugDescription}
    </Context>

    <If when={relevantCode}>
      <Context type="data" label="Code Context" priority="important" preserveFormatting>
        {relevantCode}
      </Context>
    </If>

    <Context type="situational" label="Analysis Parameters" priority="helpful">
      Focus areas: {analysisDepth}
      Bug severity: {bugSeverity}
      System-wide context: {includeSystemContext ? "Enabled - search for related patterns and similar vulnerabilities across the codebase" : "Disabled - focus investigation on the reported issue"}
    </Context>

    <If when={bugSeverity === 'critical'}>
      <Context type="situational" label="Critical Severity Protocol" priority="critical">
        This bug is marked CRITICAL. Apply emergency investigation protocol:
        - Prioritize identifying the immediate trigger to enable a tactical fix for restoration
        - Document the root cause investigation in parallel with the tactical fix
        - Clearly separate the immediate tactical fix from the strategic root cause fix
        - Include rollback guidance in case the fix introduces new issues
      </Context>
    </If>

    <Context type="reference" label="Debugging Methodology" priority="important">
    This analysis follows established systematic debugging principles:

    **The Scientific Method for Debugging:**
    1. Observe: Gather all available evidence without jumping to conclusions
    2. Hypothesize: Form specific, testable explanations for the observed behavior
    3. Experiment: Test hypotheses with the smallest possible change or probe
    4. Conclude: Accept or reject based on evidence, then iterate

    **The 5 Whys Technique:**
    Ask "why" iteratively to move from symptom to root cause. Stop when you reach a process, design, or systemic issue that can be addressed. The root cause is the deepest fixable point in the causal chain.
    *Limitations:* The 5 Whys assumes linear causality, but real failures often have multiple interacting causes. It depends heavily on practitioner knowledge -- it cannot reveal causes the team does not know about. Different people may reach different conclusions on the same problem. For complex, multi-causal failures, supplement with fault tree analysis or fishbone diagrams rather than relying on 5 Whys alone.

    **Backward Tracing:**
    Start at the error manifestation point and trace backward through the call chain, data flow, and state transitions until you find the original trigger. Fix at the source, not the symptom.

    **Fault Tree Analysis:**
    Map the logical relationships between failures. Identify which conditions were necessary (AND gates) and which were sufficient (OR gates) to produce the observed failure.

    **Binary Search / Delta Debugging:**
    When the failure point is unknown, systematically bisect the code path or change history to narrow down the location of the fault. Delta debugging automates this: reduce the failure-inducing input to the minimal set that still triggers the bug. Tools like git bisect apply this to version history.

    **Fishbone (Ishikawa) Diagram:**
    For complex, multi-causal failures where the 5 Whys is insufficient, brainstorm potential causes across structured categories: Methods (algorithms, logic), Infrastructure (hardware, cloud), Data (inputs, config, dependencies), Monitoring (logging gaps), Environment (OS, network, deployment), and Process (knowledge gaps, communication). This prevents tunnel vision by forcing consideration of diverse cause categories.

    **Explanation-Based Debugging (Rubber Duck Method):**
    Explain the code's expected behavior step by step, comparing it to actual behavior. This engages metacognition -- thinking about your own thinking -- and surfaces implicit assumptions that may be incorrect. The act of articulating forces slower, more careful examination than silent reading.

    **Guarding Against Confirmation Bias:**
    Research shows ~70% of debugging actions are affected by cognitive bias. The most dangerous is confirmation bias: seeking evidence that confirms your current hypothesis while ignoring evidence that contradicts it. Mitigation: for each hypothesis, explicitly ask "what evidence would DISPROVE this?" and actively look for it. Test multiple hypotheses, not just the first one that seems plausible.
  </Context>

  <Context type="domain" label="Common Bug Categories" priority="helpful">
    Use these categories to guide investigation:

    **Data bugs:** null/undefined access, off-by-one errors, type mismatches, encoding issues, boundary conditions, floating point precision
    **Logic bugs:** incorrect conditionals, wrong operator, missing edge case, inverted boolean, short-circuit evaluation errors
    **State bugs:** race conditions, stale state, mutation of shared state, incorrect initialization, missing cleanup
    **Integration bugs:** API contract violations, serialization mismatches, version incompatibility, configuration drift, environment differences
    **Resource bugs:** memory leaks, connection pool exhaustion, file handle leaks, deadlocks, unbounded growth
    **Timing bugs:** race conditions, timeout misconfigurations, order-of-operations errors, clock skew, retry storms
  </Context>
  </Contexts>

  <Steps style="step-by-step" verify selfCritique numbered showReasoning>
    <Step>REPRODUCE AND MINIMIZE: Confirm the bug exists and document exact conditions. Read error messages completely - they often contain the answer. Note exact error text, codes, line numbers, stack traces. Identify the expected behavior vs. actual behavior. Then create a minimal reproducible example (MRE): strip away everything not needed to trigger the failure. The act of minimizing often reveals the root cause itself, and minimal reproductions are resolved dramatically faster than complex ones. If the bug cannot be reproduced, document the conditions under which it was observed and assess whether it is intermittent (timing, race condition) or environmental. Apply the 10-minute rule: if you have spent 10 minutes debugging ad hoc without progress, stop and switch to the systematic scientific method.</Step>
    <Step>COLLECT EVIDENCE (breadth-first): Survey the full landscape of evidence before diving deep into any single theory. Expert debuggers use breadth-first exploration first, then targeted deep dives -- novices make the mistake of immediately going deep on their first hypothesis. Examine stack traces from bottom to top to find the root of the call chain. Check logs for warnings or errors preceding the failure. Review recent code changes (git diff, recent commits) that could have introduced the issue. Identify affected components, their dependencies, and data flows between them. For multi-component systems, use correlation IDs and distributed traces to follow the request across service boundaries. Leverage structured logging, metrics dashboards, and trace spans to determine where the data or state becomes invalid. Check the three pillars of observability: logs (what happened), metrics (what changed), traces (where it happened).</Step>
    <Step>ISOLATE THE FAULT: Narrow down the failure location using isolation techniques. Apply binary search debugging to bisect the code path or change history. Comment out or disable sections to identify the minimum code that triggers the failure. Check inputs and outputs at key points to find where valid data becomes invalid. Verify assumptions: are types correct, are values in expected ranges, are dependencies available, is the environment configured correctly?</Step>
    <Step>FORM HYPOTHESES: Based on evidence, generate MULTIPLE ranked hypotheses -- not just the first plausible explanation. Apply the 5 Whys starting from the observed symptom (but for complex multi-causal failures, supplement with fishbone diagrams or fault tree analysis). For each hypothesis, state it specifically: "I think [X] is the root cause because [evidence Y]." Guard against confirmation bias: for each hypothesis, explicitly identify what evidence would DISPROVE it and actively look for that evidence. Distinguish between the immediate trigger (what lit the match), contributing factors (the fuel), and the root cause (the design flaw that made it possible). Build a fault tree mapping the logical relationships (AND/OR gates).</Step>
    <Step>VERIFY HYPOTHESES: Test each hypothesis with the smallest possible change. Make ONE change at a time to test ONE hypothesis. Observe whether the behavior matches the prediction. If the hypothesis is confirmed, proceed to fix design. If refuted, record what was learned and form a new hypothesis. If three or more hypotheses fail, step back and question whether the problem is architectural rather than a localized bug.</Step>
    <Step>DESIGN THE FIX: Propose a fix that addresses the root cause, not just the symptom. Assess the risk and side effects of the proposed change. Identify whether a tactical (short-term) fix and a strategic (long-term) fix are both needed. Provide specific code changes with before/after examples. Consider alternative approaches with trade-off analysis.</Step>
    <Step>PLAN VERIFICATION AND REGRESSION TESTING: Define how to verify the fix resolves the original issue. Specify a regression test that should fail before the fix and pass after. Identify edge cases that should be tested. Recommend broader test suite execution to catch unintended side effects.</Step>
    <Step>DEVELOP PREVENTION STRATEGY: Identify the systemic weakness that allowed this bug. Recommend monitoring, logging, or alerting for early detection. Suggest process improvements (code review checklist items, test patterns, linting rules). If system-wide analysis is enabled, identify similar patterns elsewhere in the codebase.</Step>
  </Steps>

  <Format type="markdown" template={`## Root Cause Analysis

### Executive Summary
[2-3 sentence summary: what failed, why it happened at the root cause level, and the recommended fix]

### Observed Symptoms
**What the user/system experienced:**
[Exact error messages, behaviors, or failures observed]

**Expected behavior:**
[What should have happened]

**Actual behavior:**
[What actually happened, with specific details]

**Reproduction conditions:**
[Steps, environment, data, timing required to trigger the bug]

### Evidence Collected

**Error Messages and Stack Traces:**
\`\`\`
[Exact error text, codes, stack traces - annotated with significance]
\`\`\`

**Affected Components:**
| Component | Role in Failure | Evidence |
|-----------|----------------|----------|
| [component] | [how it contributes to the failure] | [specific evidence] |

**Data Flow Analysis:**
\`\`\`
[Trace data from source to failure point, marking where it becomes invalid]
Entry point → [valid] → Component A → [valid] → Component B → [INVALID HERE] → Error
\`\`\`

**Recent Changes Examined:**
[Relevant git history, deployments, configuration changes evaluated and their relevance]

### Failure Chain Analysis

**The 5 Whys:**
1. **Why** did [symptom] occur? → Because [immediate cause]
2. **Why** did [immediate cause] happen? → Because [deeper cause]
3. **Why** did [deeper cause] happen? → Because [still deeper]
4. **Why** did [still deeper] happen? → Because [contributing factor]
5. **Why** did [contributing factor] exist? → Because [ROOT CAUSE: systemic/design issue]

**Failure Chain Summary:**
- **Immediate Trigger:** [What directly caused the error - the match that lit the fire]
- **Contributing Factors:** [Conditions that made the failure possible - the fuel]
- **Root Cause:** [Underlying issue that must be fixed - the design flaw]

**Fault Tree:**
\`\`\`
[Root Cause]
├── [Contributing Factor A] (AND)
│   ├── [Condition 1]
│   └── [Condition 2]
└── [Contributing Factor B] (AND)
    └── [Condition 3: immediate trigger]
\`\`\`

### Technical Deep Dive

**Code Path to Failure:**
\`\`\`
[Annotated execution trace from entry point to failure]
\`\`\`

**Assumptions Violated:**
[What the code assumed that turned out to be false, and why it was a reasonable but incorrect assumption]

**Hypotheses Considered:**
| Hypothesis | Supporting Evidence | Disconfirming Evidence | Verdict |
|------------|-------------------|----------------------|---------|
| [hypothesis 1] | [evidence for] | [evidence against or what was checked] | [confirmed/rejected/inconclusive] |
| [hypothesis 2] | [evidence for] | [evidence against] | [confirmed/rejected/inconclusive] |

**Bug Category:** [data / logic / state / integration / resource / timing]

### Recommended Fix

**Primary Solution (Root Cause Fix):**
[Description of the fix that addresses the root cause]

\`\`\`[language]
// Before (broken):
[problematic code with annotation]

// After (fixed):
[corrected code with explanation of each change]
\`\`\`

**Risk Assessment:**
| Factor | Rating | Notes |
|--------|--------|-------|
| Complexity | [Low/Medium/High] | [explanation] |
| Side Effects | [None/Minimal/Moderate/Significant] | [what could be affected] |
| Confidence | [High/Medium/Low] | [strength of evidence] |

**Alternative Approaches:**
1. [Alternative fix with trade-offs]
2. [Another option with trade-offs]

**Verification Plan:**
1. [Unit test: specific test case that should fail before fix and pass after]
2. [Integration test: broader validation]
3. [Manual verification: steps to confirm resolution]
4. [Regression check: existing tests to re-run]

### Prevention Strategy

**Immediate Actions:**
- [ ] [Fix to deploy now]
- [ ] [Test to add for this specific bug]
- [ ] [Monitoring to enable]

**Systemic Improvements:**
- [ ] [Process change to catch similar issues earlier]
- [ ] [Code pattern or architectural improvement to prevent this class of bug]
- [ ] [Linting rule, type check, or static analysis to add]

**Monitoring and Early Detection:**
- [ ] [Logging enhancement for visibility]
- [ ] [Alert or metric to track]
- [ ] [Health check or canary to add]

### Confidence Assessment

**Overall confidence in root cause identification:** [High / Medium / Low]
**Evidence strength:** [What supports this conclusion]
**Remaining uncertainty:** [What is still unknown and what evidence would resolve it]
**Alternative explanations not fully ruled out:** [If any, with probability assessment]
`} strict validate />

  <Constraints presets={["acknowledge-uncertainty", "cite-sources"]}>
    <Constraint type="must" category="accuracy">
      Base all conclusions on verifiable evidence: error messages, logs, stack traces, code inspection, or reproduction results. Never present speculation as fact.
    </Constraint>
    <Constraint type="must" category="scope">
      Distinguish clearly between immediate triggers, contributing factors, and root causes. The root cause is the deepest fixable point in the causal chain, not just the proximate failure.
    </Constraint>
    <Constraint type="must" category="content">
      Include the complete failure chain from root cause through intermediate steps to observed symptom, showing how each link leads to the next.
    </Constraint>
    <Constraint type="must" category="accuracy">
      Apply the 5 Whys methodology to trace from symptom to root cause. Each "why" MUST be supported by evidence, not assumption.
    </Constraint>
    <Constraint type="must-not" category="accuracy" positive="State what is known, what is hypothesized, and what requires further investigation">
      Jump to conclusions without sufficient evidence or propose fixes before completing root cause investigation.
    </Constraint>
    <Constraint type="must" category="content">
      Include verification steps that confirm the fix resolves the issue without introducing side effects.
    </Constraint>
    <Constraint type="should" category="content">
      Provide multiple solution approaches when applicable, with trade-off analysis covering complexity, risk, and effort.
    </Constraint>
    <Constraint type="should" category="content">
      Reference specific line numbers, function names, variable values, and code snippets when discussing technical details.
    </Constraint>
    <Constraint type="should" category="content">
      Categorize the bug type (data, logic, state, integration, resource, timing) to help guide investigation and prevention.
    </Constraint>
    <Constraint type="must-not" category="scope" positive="Address the root cause so the fix eliminates the class of bug, not just the instance">
      Propose quick fixes that address only the symptom while leaving the underlying cause intact.
    </Constraint>
    <Constraint type="should" category="accuracy">
      Consider multi-causal explanations: real-world failures rarely have a single root cause. When evidence points to multiple interacting factors, use fault tree analysis (AND/OR gates) rather than forcing a single linear causal chain. The 5 Whys technique has known limitations with complex, multi-causal failures.
    </Constraint>
    <Constraint type="must" category="accuracy">
      Actively counter confirmation bias: for each hypothesis, explicitly state what evidence would disprove it and look for that evidence before concluding. Do not settle on the first plausible explanation without considering alternatives.
    </Constraint>

    <If provider="anthropic">
      <Constraint type="should" category="format">
        Use XML-structured thinking for complex multi-step analysis. Structure stack trace analysis and data flow tracing in clear annotated sections.
      </Constraint>
    </If>

    <If provider="openai">
      <Constraint type="should" category="format">
        Use markdown code blocks with explicit language tags for all code examples and stack traces. Use tables for structured comparison data.
      </Constraint>
    </If>

    <If provider="google">
      <Constraint type="should" category="format">
        Use structured markdown with clear heading hierarchy. Provide inline code annotations for technical analysis.
      </Constraint>
    </If>
  </Constraints>

  <Guardrails preset="standard"
    prohibit={[
      "Suggesting quick fixes without completing root cause investigation",
      "Making assumptions about code behavior without evidence from error messages, logs, or code inspection",
      "Proposing solutions that address symptoms rather than the underlying cause",
      "Claiming certainty when evidence is insufficient or contradictory",
      "Skipping the hypothesis verification step and jumping directly to fix recommendations",
      "Shotgun debugging: making undirected, random changes hoping to fix the bug without understanding the cause",
      "Changing multiple variables at once: testing more than one hypothesis per experiment makes it impossible to determine which change had the effect",
      "Tunnel vision: fixating on one code area while ignoring system-level interactions, configuration, or environmental causes"
    ]}
    require={[
      "Complete the full investigation pipeline before proposing fixes: observe, hypothesize, verify, then fix",
      "Trace failures back to their origin using backward tracing, not just to the proximate cause",
      "Distinguish between 'what broke' (immediate trigger) and 'why it was breakable' (root cause)",
      "State confidence level for each conclusion and what evidence supports it",
      "Include a regression test strategy with the fix recommendation",
      "Generate multiple hypotheses and actively seek disconfirming evidence for each (counter confirmation bias)",
      "Create or recommend creating a minimal reproducible example before deep-diving into root cause analysis"
    ]}
  />

  <EdgeCases preset="standard">
    <When condition="error is not reproducible consistently"
      then="Document the conditions under which it does and does not occur. Analyze for timing dependencies (race conditions, timeouts), environmental factors (OS, memory pressure, network latency), or data-dependent triggers. Recommend instrumentation: add logging at component boundaries, capture state snapshots, and use structured logging with correlation IDs. Consider whether the bug is a Heisenbug that changes behavior under observation." />
    <When condition="multiple potential root causes identified with similar evidence"
      then="Rank each hypothesis by: (1) strength of supporting evidence, (2) consistency with all observed symptoms, (3) parsimony (simpler explanations preferred). Design a discriminating test for each that would confirm one while ruling out others. Present all candidates with confidence levels." />
    <When condition="stack trace or error message is cryptic, misleading, or absent"
      then="Focus on reproduction and observable behavior rather than the error text. Trace code execution manually using the code structure. Recommend adding diagnostic logging at key decision points. Explain what the error message actually indicates versus what it appears to indicate." />
    <When condition="bug only occurs in production or a specific environment"
      then="Systematically compare environment configurations, dependency versions, data characteristics, load patterns, and system resources. Check for configuration drift between environments. Recommend creating a staging environment that replicates production conditions, or adding production-safe diagnostic instrumentation." />
    <When condition="recent code changes are not obvious contributors"
      then="Expand the investigation timeline: check dependency updates, infrastructure changes, data migrations, configuration modifications, certificate expirations, and external service changes. Consider whether a latent bug was exposed by a change in usage patterns or data volume." />
    <When condition="fix requires significant refactoring or architecture changes"
      then="Provide two solutions: (1) a tactical short-term fix that mitigates the immediate issue with minimal risk, and (2) a strategic long-term fix that addresses the architectural root cause. Include effort estimates and risk assessments for each. Note technical debt implications of choosing the tactical fix." />
    <When condition="three or more fix attempts have already failed"
      then="Stop proposing incremental fixes. Step back and question whether the problem is architectural rather than a localized bug. Look for: shared state coupling, incorrect design assumptions, or fundamental pattern mismatch. Recommend a design review before further fix attempts." />
    <When condition="bug involves concurrency, threading, or async behavior"
      then="Map the concurrent execution paths. Identify shared mutable state, synchronization points, and ordering assumptions. Check for: missing locks, lock ordering violations, time-of-check-time-of-use (TOCTOU) errors, callback ordering assumptions, and promise/async-await error handling gaps. Recommend tools: thread sanitizers, async stack traces, structured concurrency analysis." />
    <When condition="bug is a regression from a previously known-good state"
      then="Apply delta debugging: use git bisect or equivalent to binary search the commit history and identify the exact change that introduced the regression. Compare the known-good state against the current failing state. Focus the investigation on what changed between the two states: code, configuration, dependencies, or data. This approach is O(log n) and dramatically faster than examining all recent changes." />
    <When condition="investigation is stuck or going in circles"
      then="Apply explanation-based debugging: explain the code's expected behavior step by step to a colleague (or rubber duck). This engages metacognition and surfaces implicit assumptions. Also try inverting the problem: instead of asking 'why does it fail?' ask 'why would it ever work?' or 'what conditions must be true for this to succeed?' Review whether confirmation bias is at play -- are you only seeking evidence for your preferred hypothesis while ignoring contradictions?" />
  </EdgeCases>

  <Fallbacks preset="standard">
    <Fallback when="cannot reproduce the bug with provided information"
      then="List the specific additional information needed to proceed: exact reproduction steps, environment details (OS, runtime versions, configuration), input data samples, complete log output, and timing information. Explain why reproduction is a prerequisite for confident root cause analysis, and suggest alternative investigation approaches (log analysis, code review for obvious defects) that can proceed without reproduction." />
    <Fallback when="root cause analysis reaches the limit of available information"
      then="Document clearly: (1) what is known with evidence, (2) what remains unknown, (3) what the most likely hypothesis is with confidence level and supporting reasoning, (4) what specific evidence would resolve the uncertainty, and (5) recommended next steps for gathering that evidence." />
    <Fallback when="recommended fix is outside the scope of the code provided"
      then="Describe what components need to be examined or modified, what interfaces are involved, and provide architectural guidance for the change. Identify the minimum additional context needed to provide a concrete fix." />
    <Fallback when="the bug appears to be in a third-party library or external dependency"
      then="Verify the dependency version and check for known issues in the dependency's issue tracker. Determine whether the bug is in the dependency itself or in how it is being used. Provide workaround options: pinning versions, monkey-patching, wrapping with defensive code, or switching to an alternative." />
  </Fallbacks>

  <WhenUncertain action="acknowledge">
    If you cannot determine the root cause with certainty, follow this protocol:
    1. State your confidence level explicitly (high / medium / low)
    2. Present the strongest hypothesis with its supporting evidence
    3. List what evidence would increase certainty
    4. Provide ranked alternative explanations if applicable
    5. Recommend specific investigative steps to resolve the uncertainty

    Never present a hypothesis as a definitive conclusion. The difference between "this IS the root cause" and "this is LIKELY the root cause based on [evidence], with [caveats]" is critical for accurate debugging.
  </WhenUncertain>

  <Examples>
    <Example>
      <ExampleInput>{`
        Bug: "Application crashes with 'Cannot read property 'length' of undefined' in dashboard.tsx line 45"

        Code:
        \`\`\`typescript
        function DashboardStats({ userData }) {
          const avgScore = userData.scores.reduce((a,b) => a+b) / userData.scores.length;
          return &lt;div&gt;{avgScore}&lt;/div&gt;;
        }
        \`\`\`
`}      </ExampleInput>
      <ExampleOutput>{`
        ## Root Cause Analysis

        ### Executive Summary
        The dashboard crashes because \`userData.scores\` is undefined for users without score history. The root cause is a missing data contract between the API layer and the UI component: the component assumes all user objects have a \`scores\` array, but the API returns partial user objects for new accounts.

        ### Observed Symptoms
        **What the user/system experienced:** Application crash on dashboard page load
        **Expected behavior:** Dashboard displays average score or a meaningful default for users without scores
        **Actual behavior:** Uncaught TypeError: Cannot read property 'length' of undefined at dashboard.tsx:45

        ### Evidence Collected
        **Error Messages and Stack Traces:**
        \`\`\`
        TypeError: Cannot read property 'length' of undefined
          at DashboardStats (dashboard.tsx:45)
        \`\`\`
        The error occurs on \`userData.scores.length\`, indicating \`userData.scores\` is undefined.

        **Data Flow Analysis:**
        \`\`\`
        API response → [partial user object without scores] → DashboardStats component → [CRASH: .scores is undefined]
        \`\`\`

        ### Failure Chain Analysis

        **The 5 Whys:**
        1. **Why** did the app crash? → Because \`userData.scores.length\` threw TypeError on undefined
        2. **Why** was \`userData.scores\` undefined? → Because the API returns user objects without a \`scores\` array for new accounts
        3. **Why** doesn't the component handle missing scores? → Because it was developed against seed data that always included scores
        4. **Why** wasn't this caught in testing? → Because test fixtures did not include the "new user without scores" state
        5. **Why** was this state not in test fixtures? → **ROOT CAUSE:** No data contract defines which fields are guaranteed vs. optional on user objects

        **Failure Chain Summary:**
        - **Immediate Trigger:** Accessing \`.length\` on undefined value
        - **Contributing Factor:** API returns partial objects without scores array for new users
        - **Root Cause:** Missing data contract between API and UI; component assumes fields that are not guaranteed

        **Bug Category:** data (null/undefined access due to implicit assumption about data shape)

        ### Recommended Fix

        **Primary Solution:**
        \`\`\`typescript
        // Before (broken):
        function DashboardStats({ userData }) {
          const avgScore = userData.scores.reduce((a,b) => a+b) / userData.scores.length;
          return &lt;div&gt;{avgScore}&lt;/div&gt;;
        }

        // After (fixed):
        function DashboardStats({ userData }: { userData: UserData }) {
          const scores = userData?.scores ?? [];
          const avgScore = scores.length > 0
            ? scores.reduce((a, b) => a + b, 0) / scores.length
            : 0;
          return &lt;div&gt;{avgScore}&lt;/div&gt;;
        }
        \`\`\`

        **Verification Plan:**
        1. Unit test: render DashboardStats with \`{ scores: undefined }\` - should display 0
        2. Unit test: render with \`{ scores: [] }\` - should display 0
        3. Unit test: render with \`{ scores: [80, 90] }\` - should display 85 (regression)
        4. Integration: load dashboard as a new user with no score history

        ### Prevention Strategy
        **Immediate:** Enable \`strictNullChecks: true\` in tsconfig.json
        **Systemic:** Define a TypeScript interface for UserData with optional fields marked explicitly. Audit all data access patterns for similar implicit assumptions. Add test fixtures for empty/partial/new-user states.
`}      </ExampleOutput>
    </Example>

    <NegativeExample reason="Jumps to fix without investigation, no failure chain, no evidence analysis, addresses symptom not cause">{`
      The code crashes because of an undefined error. To fix it, wrap everything in a try-catch block:
      \`\`\`typescript
      try {
        const avgScore = userData.scores.reduce((a,b) => a+b) / userData.scores.length;
      } catch (e) {
        return &lt;div&gt;0&lt;/div&gt;;
      }
      \`\`\`
      This will prevent the crash.
`}    </NegativeExample>

    <NegativeExample reason="No evidence for the hypothesis, wrong root cause, no reproduction, no verification steps">{`
      The error happens because of a race condition in the data fetching. The API call returns after the component renders, so the data isn't ready yet. You should add async/await and a loading state, and that will fix it.
`}    </NegativeExample>
  </Examples>

  <Audience
    level="advanced"
    type="technical"
    knowledgeLevel="Professional software engineers familiar with debugging concepts and development tooling"
    goals={[
      "understand the true root cause, not just symptoms",
      "implement a fix that prevents recurrence of this bug and similar bugs",
      "learn from the failure to improve the system's resilience",
      "build confidence that the diagnosis is correct before investing in the fix"
    ]}
  />

  <Tone type="professional" formality="semi-formal" energy="measured"
    avoidTones={["alarmist", "dismissive", "overconfident"]}
  />

  <Style type="technical" verbosity="moderate" formality="semi-formal" />

  <SuccessCriteria>
    <Criterion category="accuracy" weight="critical" metric="root cause supported by at least 2 pieces of evidence">
      Root cause correctly identified with supporting evidence from error messages, code analysis, or reproduction - not guesswork
    </Criterion>
    <Criterion category="completeness" weight="critical" metric="5 Whys depth of at least 3 levels">
      Complete failure chain traced from root cause through intermediate failures to observed symptom using the 5 Whys or equivalent methodology
    </Criterion>
    <Criterion category="accuracy" weight="critical">
      Clear distinction between immediate trigger (proximate cause), contributing factors, and underlying root cause
    </Criterion>
    <Criterion category="clarity" weight="critical" metric="fix includes before/after code and verification steps">
      Fix recommendations are actionable: specific code changes, risk assessment, and step-by-step verification plan
    </Criterion>
    <Criterion category="completeness" weight="important">
      Prevention strategy addresses both the immediate fix and systemic improvements to prevent the class of bug
    </Criterion>
    <Criterion category="relevance" weight="important">
      Analysis is grounded in verifiable evidence with explicit confidence levels, not speculation or assumptions
    </Criterion>
    <Criterion category="completeness" weight="important" metric="at least 1 regression test specified">
      Regression test strategy included: at minimum, one test that fails before the fix and passes after
    </Criterion>
    <Criterion category="format" weight="important" metric="all template sections present and filled">
      Output follows the specified template structure with all sections completed
    </Criterion>
    <Criterion category="accuracy" weight="important" metric="at least 2 hypotheses evaluated with disconfirming evidence">
      Multiple hypotheses considered with disconfirming evidence sought for each, not just the first plausible explanation accepted
    </Criterion>
  </SuccessCriteria>

  <References sources={[
    {
      title: "A Guide to Root Cause Analysis: Solving Bugs at Their Source",
      url: "https://bugasura.io/blog/root-cause-analysis-for-bug-tracking/",
      description: "5 Whys, Fishbone Diagrams, and Fault Tree Analysis techniques for software bugs"
    },
    {
      title: "A systematic approach to debugging",
      url: "https://ntietz.com/blog/how-i-debug-2023/",
      description: "Scientific method applied to debugging: observe, hypothesize, experiment, conclude"
    },
    {
      title: "MIT 6.031 Reading: Debugging",
      url: "http://web.mit.edu/6.031/www/fa17/classes/13-debugging/",
      description: "Academic foundations of systematic debugging methodology"
    },
    {
      title: "10 debugging techniques we rely on",
      url: "https://wearebrain.com/blog/10-effective-debugging-techniques-for-developers/",
      description: "Hypothesis-driven debugging, binary search, and isolation techniques"
    },
    {
      title: "Incident Review and Postmortem Best Practices",
      url: "https://blog.pragmaticengineer.com/postmortem-best-practices/",
      description: "Industry standard postmortem methodology from The Pragmatic Engineer"
    },
    {
      title: "Google SRE: Postmortem Culture",
      url: "https://sre.google/sre-book/postmortem-culture/",
      description: "Blameless postmortem methodology and organizational learning from failures"
    },
    {
      title: "Why Programs Fail: A Guide to Systematic Debugging",
      url: "https://dl.acm.org/doi/10.5555/1718010",
      description: "Andreas Zeller's foundational work on scientific debugging and delta debugging methodology"
    },
    {
      title: "Cognitive Biases in Software Development",
      url: "https://cacm.acm.org/research/cognitive-biases-in-software-development/",
      description: "ACM research on how confirmation bias and other cognitive biases affect debugging effectiveness"
    },
    {
      title: "Delta Debugging: Simplifying and Isolating Failure-Inducing Input",
      url: "https://www.debuggingbook.org/html/DeltaDebugger.html",
      description: "Automated test case reduction and minimal reproduction techniques"
    }
  ]} />

  <ChainOfThought style="structured" showReasoning />
</Prompt>
